{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68113967-01c8-4bec-95f1-e872f572e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import (Trainer, TrainingArguments, DataCollatorWithPadding,\n",
    "                          AutoTokenizer, AutoModelForSequenceClassification)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17981e0a-7ae5-4aba-81f3-405f4bf48717",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../../../data/spancat/'\n",
    "model_name_or_path = 'microsoft/deberta-v3-large'\n",
    "dataset_path = DATA+'strategies-ds.hf'\n",
    "output_dir = DATA+'spancat/results/metacognitive-cls'\n",
    "model_max_length = 2056\n",
    "dataframe = '~/data/spancat/strategies-df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b73702d-4a51-4e9b-8e9c-f383371869f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataframe).dropna()\n",
    "classes = list(df.columns[1:])\n",
    "class2id = {class_:id for id, class_ in enumerate(classes)}\n",
    "id2class = {id:class_ for id, class_ in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502d6d8b-1985-4945-a60c-fb0c211c1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    ds = datasets.load_from_disk(dataset_path)\n",
    "    return ds\n",
    "ds = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8e64bf-d1d0-40df-9307-41e9822290e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1/(1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   predictions, labels = eval_pred\n",
    "   predictions = sigmoid(predictions)\n",
    "   predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "   return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3162a43-8ec5-41db-b455-aeba973cafd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda_envs/wes-env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    max_length=model_max_length,\n",
    "    )\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=16, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a990d4ce-6ef5-4a0a-bcea-5b38c2b74fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "learning_rate = 9.946303722432942e-06\n",
    "warmup_steps = 500\n",
    "weight_decay = 0.01\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57bf461-9cfb-4609-8bb4-98f0e4a24a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtiedaar1\u001b[0m (\u001b[33mai-aloe\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762f78b220844248b5927d8a6226719c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111352268813385, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/metacognitive-span-categorization/src/classification/wandb/run-20240522_203544-dmxgmf1s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai-aloe/huggingface/runs/dmxgmf1s' target=\"_blank\">laced-smoke-78</a></strong> to <a href='https://wandb.ai/ai-aloe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai-aloe/huggingface' target=\"_blank\">https://wandb.ai/ai-aloe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai-aloe/huggingface/runs/dmxgmf1s' target=\"_blank\">https://wandb.ai/ai-aloe/huggingface/runs/dmxgmf1s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [732/732 09:29, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.660293</td>\n",
       "      <td>0.634221</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>0.164154</td>\n",
       "      <td>0.313099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.526900</td>\n",
       "      <td>0.379402</td>\n",
       "      <td>0.871926</td>\n",
       "      <td>0.505929</td>\n",
       "      <td>0.663212</td>\n",
       "      <td>0.408946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.340934</td>\n",
       "      <td>0.877049</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.755245</td>\n",
       "      <td>0.345048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.308118</td>\n",
       "      <td>0.893955</td>\n",
       "      <td>0.558635</td>\n",
       "      <td>0.839744</td>\n",
       "      <td>0.418530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.275621</td>\n",
       "      <td>0.902152</td>\n",
       "      <td>0.589247</td>\n",
       "      <td>0.901316</td>\n",
       "      <td>0.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.255029</td>\n",
       "      <td>0.912910</td>\n",
       "      <td>0.680451</td>\n",
       "      <td>0.826484</td>\n",
       "      <td>0.578275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.238818</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.806084</td>\n",
       "      <td>0.677316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.205888</td>\n",
       "      <td>0.927766</td>\n",
       "      <td>0.741284</td>\n",
       "      <td>0.870690</td>\n",
       "      <td>0.645367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.206372</td>\n",
       "      <td>0.922131</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.765677</td>\n",
       "      <td>0.741214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.178622</td>\n",
       "      <td>0.940061</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.824503</td>\n",
       "      <td>0.795527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.158904</td>\n",
       "      <td>0.945697</td>\n",
       "      <td>0.822742</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.785942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.154398</td>\n",
       "      <td>0.947746</td>\n",
       "      <td>0.827703</td>\n",
       "      <td>0.878136</td>\n",
       "      <td>0.782748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=732, training_loss=0.27466803170292753, metrics={'train_runtime': 602.0839, 'train_samples_per_second': 19.393, 'train_steps_per_second': 1.216, 'total_flos': 4128159774807552.0, 'train_loss': 0.27466803170292753, 'epoch': 12.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "''' The main training loop.\n",
    "'''\n",
    "# wandb.init()\n",
    "\n",
    "# config = wandb.config\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path,\n",
    "                                                           num_labels=8,\n",
    "                                                           id2label=id2class, \n",
    "                                                           label2id=class2id, \n",
    "                                                           problem_type = \"multi_label_classification\")\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_steps = warmup_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1972ebec-d335-49b3-99b3-59ee8e1236ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49b67363-b3c9-42d6-879c-1d9d9c10cae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tiedaar/training1/commit/b306f94e30d959e60793d90d0c7d003832bdb4b0', commit_message='metacognitive_strategy_cls', commit_description='', oid='b306f94e30d959e60793d90d0c7d003832bdb4b0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub('metacognitive_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4ce7845-b8cd-44a8-9b63-a8eae189b8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b137d5fa6e47759161fccf03b0a301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tiedaar/metacognitive_strategy_cls/commit/c06842978b11b5f08f1d1e8c68e1e42c4f770dbb', commit_message='Upload tokenizer', commit_description='', oid='c06842978b11b5f08f1d1e8c68e1e42c4f770dbb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('metacognitive_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af1c98e7-54e7-412d-80e2-a2b56890a241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7c50bdf-b207-4157-bf44-03da41924b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "82485fbf-030b-4a50-841b-81f8e3720627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For this exam, I started by reading the assign...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To prepare for this exam I carefully read the ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I read through the textbook using the study gu...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I began studying for the exam by reviewing my ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I read the textbook after lectures. I wrote do...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Took notes on each lecture while reviewing the...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Prior to the classes, I read all of the requir...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>To study for this exam I read over all of the ...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>I went through the slides and filled out the s...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>For this exam, I first created a study guide u...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    For this exam, I started by reading the assign...   \n",
       "1    To prepare for this exam I carefully read the ...   \n",
       "2    I read through the textbook using the study gu...   \n",
       "3    I began studying for the exam by reviewing my ...   \n",
       "4    I read the textbook after lectures. I wrote do...   \n",
       "..                                                 ...   \n",
       "239  Took notes on each lecture while reviewing the...   \n",
       "240  Prior to the classes, I read all of the requir...   \n",
       "241  To study for this exam I read over all of the ...   \n",
       "242  I went through the slides and filled out the s...   \n",
       "243  For this exam, I first created a study guide u...   \n",
       "\n",
       "                                       labels                     preds  \n",
       "0    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]  [0, 0, 0, 1, 0, 0, 0, 1]  \n",
       "1    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]  [0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]  [0, 0, 0, 1, 0, 0, 0, 1]  \n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  [0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "..                                        ...                       ...  \n",
       "239  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "240  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]  [0, 0, 0, 1, 0, 0, 0, 1]  \n",
       "241  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "242  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]  [0, 0, 0, 1, 0, 0, 0, 1]  \n",
       "243  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]  [0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "\n",
       "[244 rows x 3 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "   return 1/(1 + np.exp(-x))\n",
    "    \n",
    "def generate_output(sequence):\n",
    "    input_ids = tokenizer(sequence, return_tensors='pt')['input_ids']\n",
    "    outputs = np.array(model(input_ids).logits.detach().reshape(-1))\n",
    "    predictions = sigmoid(outputs)\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "    return predictions\n",
    "\n",
    "test_df = pd.DataFrame(ds['test'])[['text', 'labels']]\n",
    "test_df['preds'] = test_df['text'].apply(lambda x: generate_output(x))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "daad5034-dca8-4a0d-9517-15afbaa8cb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For this exam, I started by reading the assigned textbook chapters. I took note of vocabulary words and their definitions. I also took note of concepts that I wasnâ€™t already familiar with with. Later, I went through the study guide and answered the questions listed. Finally, I used the practice exam to test my knowledge.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5ebc6737-9df6-46a4-aad0-ebc4d133ffb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9431352459016393"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3377feee-c753-4f48-af00-4332d920717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1/(1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   predictions, labels = eval_pred\n",
    "   predictions = sigmoid(predictions)\n",
    "   predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "   return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))\n",
    "\n",
    "\n",
    "sequence = ds['test']['text'][i]\n",
    "labels = ds['test']['labels'][i]\n",
    "print(labels)\n",
    "input_ids = tokenizer(sequence, return_tensors='pt')\n",
    "outputs = model(input_ids['input_ids']).logits\n",
    "predictions = sigmoid(np.array(outputs.detach().reshape(-1)))\n",
    "predictions = (predictions>0.5).astype(int)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wes-env]",
   "language": "python",
   "name": "conda-env-wes-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
